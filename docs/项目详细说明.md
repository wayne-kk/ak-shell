# A股数据采集系统 - 详细技术说明

## 🏗️ 项目概览

本项目是一个**基于 AKShare + Supabase + Python 的 A股数据采集与管理系统**，专注于股票数据的自动化采集、存储和通知。系统采用模块化设计，支持历史数据批量采集、增量更新、定时任务调度和智能通知。

### 核心特性
- ✅ **多数据源采集**: 股票行情、基础信息、新闻、资金流向、人气榜等
- ✅ **智能调度**: 支持 Python 内置调度器和 Crontab 两种定时任务模式
- ✅ **数据质量保证**: 重试机制、异常处理、数据清洗和去重
- ✅ **实时通知**: 飞书 Webhook 通知任务执行状态和数据统计
- ✅ **断点续传**: 支持历史数据采集中断后续传
- ✅ **云端存储**: Supabase PostgreSQL 数据库存储

## 🏛️ 系统架构

### 技术栈
```
Frontend:     无（纯后端数据采集系统）
Backend:      Python 3.8+
Database:     Supabase (PostgreSQL)
Data Source:  AKShare API
ORM:          Prisma (数据模型定义)
Scheduler:    APScheduler / Crontab
Notification: 飞书 Webhook
Monitoring:   Loguru (日志系统)
```

### 项目结构
```
ak-shell/
├── src/                    # 核心源码
│   ├── main.py            # 主程序入口
│   ├── scheduler.py       # Python内置定时调度器
│   ├── config.py          # 统一配置管理
│   ├── database.py        # Supabase数据库操作
│   ├── collectors.py      # 数据采集器（核心模块）
│   ├── feishu_notify.py   # 飞书通知模块
│   ├── utils/             # 工具模块
│   │   ├── logger.py      # 日志配置
│   │   ├── date_utils.py  # 日期工具
│   │   ├── file_utils.py  # 文件操作
│   │   └── retry.py       # 重试装饰器
│   └── api/               # API接口（预留）
├── docs/                  # 文档目录
├── logs/                  # 日志文件
├── scripts/               # 脚本工具
├── prisma/                # 数据库Schema
├── run.py                 # 统一CLI入口
├── requirements.txt       # Python依赖
└── README.md              # 项目说明
```

## 🔧 核心模块详解

### 1. 配置管理 (`config.py`)
**职责**: 统一管理所有系统配置
```python
class Config:
    # 数据库配置
    SUPABASE_URL = os.getenv('SUPABASE_URL')
    SUPABASE_SERVICE_ROLE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
    
    # AKShare配置
    AKSHARE_TIMEOUT = int(os.getenv('AKSHARE_TIMEOUT', 30))
    AKSHARE_RETRY_COUNT = int(os.getenv('AKSHARE_RETRY_COUNT', 3))
    AKSHARE_RATE_LIMIT = int(os.getenv('AKSHARE_RATE_LIMIT', 10))
    
    # 采集延时配置（防止被限流）
    DELAY_CONFIG = {
        'base_delay': 0.1,      # 基础延时
        'random_delay': 0.2,    # 随机延时
        'batch_delay': 2.0,     # 批次延时
        'batch_size': 50        # 批次大小
    }
```

### 2. 数据库操作 (`database.py`)
**职责**: Supabase数据库连接和CRUD操作
```python
class Database:
    # 核心方法
    - insert_dataframe()    # DataFrame批量插入
    - upsert_dataframe()    # 插入或更新（处理重复数据）
    - get_latest_date()     # 获取最新数据日期（用于断点续传）
    - get_stock_list()      # 分页获取股票列表
    - count_records()       # 记录计数
    - execute_query()       # 通用查询
```

**特点**:
- 支持分批处理大数据集（1000条/批）
- 自动处理 NaN 值和数据类型转换
- 内置重试机制和异常处理

### 3. 数据采集器 (`collectors.py`)
**核心模块**，包含多个专业化采集器：

#### 基础采集器 (`BaseCollector`)
```python
class BaseCollector:
    # 智能延时控制（防止API限流）
    def smart_delay(self, index: int = 0)
    
    # 安全API请求（带重试）
    @retry(tries=3, delay=5)
    def safe_request(self, func, *args, **kwargs)
    
    # 数据清理
    def clean_dataframe(self, df: pd.DataFrame)
```

#### 专业采集器
1. **StockBasicCollector**: 股票基础信息
   - 数据源：东财股票列表 API
   - 功能：股票代码、名称、交易所、ST判断
   - 更新频率：每周

2. **DailyQuoteCollector**: 日线行情数据
   - 数据源：AKShare 股票日线数据
   - 功能：历史行情、最新价格、批量更新
   - 特点：支持断点续传、批量采集优化

3. **IndexDataCollector**: 指数数据
   - 数据源：AKShare 指数历史数据
   - 涵盖：上证指数、深证成指、创业板指等
   - 特点：支持数据更新检测和重试

4. **StockNewsCollector**: 股票新闻
   - 数据源：东方财富全球财经快讯
   - 功能：新闻采集、去重、定时清理
   - 特点：支持关键词搜索、标签过滤

5. **资金流向采集器**: 
   - `HsgtFundFlowCollector`: 沪深港通资金流向
   - `StockFundFlowRankCollector`: 个股资金流排名

6. **热门数据采集器**:
   - `StockHotRankCollector`: 股票人气榜
   - `StockHotUpCollector`: 股票飙升榜

### 4. 调度系统

#### Python内置调度器 (`scheduler.py`)
```python
def setup_schedules():
    # 每日18:00: 股票数据更新（收盘后）
    schedule.every().day.at("18:00").do(job_daily_update)
    
    # 每周日02:00: 基础信息更新
    schedule.every().sunday.at("02:00").do(job_weekly_update)
    
    # 每20分钟: 新闻数据采集
    schedule.every(20).minutes.do(job_news_update)
    
    # 每小时: 健康检查
    schedule.every().hour.do(job_health_check)
```

**优势**:
- 程序内部调度，共享内存和连接
- 实时监控和日志
- 异常处理和飞书通知
- 可动态调整调度

#### Crontab 调度（传统方式）
```bash
# 每日18:30执行数据采集
30 18 * * * cd /path/to/ak-shell && python3 src/main.py today

# 每周日02:00更新基础信息
0 2 * * 0 cd /path/to/ak-shell && python3 -c "from src.collectors import stock_basic_collector; stock_basic_collector.collect()"
```

### 5. 通知系统 (`feishu_notify.py`)
**职责**: 飞书 Webhook 消息推送

```python
class FeishuNotifier:
    # 文本消息
    def send_text_message(self, content: str)
    
    # 卡片消息（支持富文本）
    def send_card_message(self, title: str, content: Dict)
    
    # 任务完成通知
    def notify_task_completion(self, task_name: str, success: bool, details: Dict)
    
    # 每日汇总报告
    def notify_daily_summary(self, summary_data: Dict)
```

**消息类型**:
- ✅ 任务成功/失败通知
- 📊 每日数据采集汇总
- ⚠️ 异常情况告警
- 📈 数据统计报告

### 6. 日志系统 (`utils/logger.py`)
**特点**:
- 基于 Loguru，支持彩色控制台输出
- 自动按日期轮转日志文件
- 支持多个日志实例（按模块分离）
- 统一的日志格式和配置

## 🗃️ 数据模型

### 核心数据表
1. **stock_basic**: 股票基础信息
   ```sql
   stock_code    VARCHAR  # 股票代码
   stock_name    VARCHAR  # 股票名称
   exchange      VARCHAR  # 交易所
   is_st         BOOLEAN  # 是否ST
   status        VARCHAR  # 状态
   update_time   TIMESTAMP
   ```

2. **daily_quote**: 日线行情
   ```sql
   stock_code    VARCHAR  # 股票代码
   trade_date    DATE     # 交易日期
   open_price    DECIMAL  # 开盘价
   high_price    DECIMAL  # 最高价
   low_price     DECIMAL  # 最低价
   close_price   DECIMAL  # 收盘价
   volume        BIGINT   # 成交量
   amount        DECIMAL  # 成交额
   ```

3. **stock_news**: 新闻数据
   ```sql
   news_id       VARCHAR  # 新闻ID
   title         VARCHAR  # 标题
   content       TEXT     # 内容
   pub_time      TIMESTAMP # 发布时间
   source        VARCHAR  # 来源
   url           VARCHAR  # 链接
   ```

### 数据流向
```
AKShare API → 数据采集器 → 数据清理 → Supabase数据库 → 飞书通知
```

## 🚀 使用方式

### 命令行接口 (`run.py`)
```bash
# 股票数据采集
python3 run.py stock today              # 当日完整数据
python3 run.py stock history --start-date 20230101 --end-date 20231231

# 新闻数据采集
python3 run.py news collect            # 采集新闻
python3 run.py news stats              # 统计信息

# 系统管理
python3 run.py system config           # 查看配置
python3 run.py system logs             # 查看日志
```

### 主程序 (`src/main.py`)
```bash
# 历史数据采集
python3 src/main.py history --start-date 20230101 --end-date 20231231

# 当日数据采集（完整版，包含指数+通知）
python3 src/main.py today

# 仅股票数据（用于定时任务，不含指数）
python3 src/main.py stocks
```

### 调度器
```bash
# 启动Python内置调度器
python3 src/scheduler.py

# 设置Crontab调度
./scripts/setup_crontab.sh
```

## ⚙️ 环境配置

### 必需环境变量 (.env)
```bash
# Supabase数据库（必需）
SUPABASE_URL="https://your-project.supabase.co"
SUPABASE_SERVICE_ROLE_KEY="your-service-key"

# 飞书通知（可选）
FEISHU_WEBHOOK_URL="https://open.feishu.cn/open-apis/bot/v2/hook/your-token"

# 采集配置（可选，有默认值）
AKSHARE_TIMEOUT=30
AKSHARE_RETRY_COUNT=3
BASE_DELAY=0.1
RANDOM_DELAY=0.2
BATCH_DELAY=2.0
BATCH_SIZE=50

# 新闻配置（可选）
NEWS_COLLECTION_INTERVAL=20
NEWS_MAX_PROCESS_COUNT=10
NEWS_CLEANUP_DAYS=7
```

## 🔍 最佳实践

### 1. 数据采集策略
- **历史数据**: 使用 `main.py history` 批量采集，支持断点续传
- **增量更新**: 使用 `main.py today` 每日更新
- **新闻数据**: 每20分钟采集10条，自动去重和清理

### 2. 错误处理
- **API限流**: 智能延时控制，批次间暂停
- **网络异常**: 自动重试机制（3次）
- **数据异常**: 数据清理和验证
- **任务失败**: 飞书通知和日志记录

### 3. 性能优化
- **批量操作**: 数据库批量插入（1000条/批）
- **分页查询**: 大表分页读取
- **内存管理**: 及时清理DataFrame
- **连接复用**: 单例数据库连接

### 4. 监控与维护
- **日志监控**: 按日期轮转，保留30天
- **通知监控**: 飞书实时状态通知
- **数据质量**: 定期检查数据完整性
- **系统健康**: 每小时健康检查

## 🛠️ 开发指南

### 添加新的数据采集器
1. **继承 BaseCollector**
2. **实现 collect() 方法**
3. **添加到 collectors.py**
4. **在 main.py 中集成调用**

```python
class NewDataCollector(BaseCollector):
    def collect(self) -> bool:
        try:
            # 1. 调用API获取数据
            data = self.safe_request(ak.some_api)
            
            # 2. 数据清理
            df = self.clean_dataframe(data)
            
            # 3. 插入数据库
            success = db.upsert_dataframe(df, 'table_name', ['key_column'])
            
            return success
        except Exception as e:
            logger.error(f"采集失败: {e}")
            return False
```

### 添加新的定时任务
1. **在 scheduler.py 中定义任务函数**
2. **在 setup_schedules() 中注册**
3. **添加飞书通知**

```python
def job_new_task():
    logger.info("开始执行新任务")
    try:
        success = new_data_collector.collect()
        if success:
            send_completion_notice("新任务", True, {"详情": "执行成功"})
    except Exception as e:
        logger.error(f"新任务失败: {e}")
        send_completion_notice("新任务", False, {"错误": str(e)})

# 注册定时任务
schedule.every().day.at("09:00").do(job_new_task)
```

## 🔧 故障排查

### 常见问题
1. **Supabase连接失败**: 检查URL和Service Role Key
2. **API限流**: 调整延时配置，减小批次大小
3. **数据重复**: 使用upsert_dataframe而非insert_dataframe
4. **内存不足**: 增加分批处理，及时清理DataFrame
5. **日志过大**: 检查日志轮转配置

### 调试技巧
```bash
# 测试数据库连接
python3 -c "from src.database import db; print('连接成功' if db.supabase else '连接失败')"

# 测试飞书通知
python3 -c "from src.feishu_notify import send_completion_notice; send_completion_notice('测试', True)"

# 查看最新日志
tail -f logs/scheduler_$(date +%Y-%m-%d).log
```

## 📝 更新日志

### 当前版本特性
- ✅ 完整的股票数据采集流程
- ✅ Python内置调度器
- ✅ 飞书通知集成
- ✅ 新闻数据采集
- ✅ 资金流向和热门数据
- ✅ 统一配置管理
- ✅ 断点续传功能

### 后续规划
- 🔄 Web管理界面
- 🔄 更多数据源集成
- 🔄 数据分析和可视化
- 🔄 API接口开发
- 🔄 Docker容器化部署

---

> **注意**: 本文档会随着项目发展持续更新，建议开发时参考最新版本。 