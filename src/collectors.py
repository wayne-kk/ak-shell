#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é‡‡é›†å™¨æ¨¡å—
"""

import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, List, Dict
from loguru import logger
import time
from retry import retry
from database import db
import random


class BaseCollector:
    """åŸºç¡€é‡‡é›†å™¨"""
    
    def __init__(self):
        self.retry_count = 3
        self.retry_delay = 5
        # å»¶æ—¶æ§åˆ¶é…ç½® - æ›´ä¿å®ˆçš„è®¾ç½®
        self.base_delay = 0.5   # åŸºç¡€å»¶æ—¶å¢åŠ åˆ°0.5ç§’
        self.random_delay = 0.8  # éšæœºå»¶æ—¶å¢åŠ åˆ°0.8ç§’
        self.batch_delay = 10.0   # æ‰¹æ¬¡é—´å»¶æ—¶å¢åŠ åˆ°10ç§’
        self.batch_size = 50     # æ‰¹æ¬¡å¤§å°å‡å°‘åˆ°50
    
    def set_delay_config(self, base_delay: float = 0.2, random_delay: float = 0.3, 
                        batch_delay: float = 2.0, batch_size: int = 100):
        """è®¾ç½®å»¶æ—¶é…ç½®"""
        self.base_delay = base_delay
        self.random_delay = random_delay
        self.batch_delay = batch_delay
        self.batch_size = batch_size
        logger.info(f"å»¶æ—¶é…ç½®å·²æ›´æ–°: åŸºç¡€å»¶æ—¶={base_delay}s, éšæœºå»¶æ—¶={random_delay}s, æ‰¹æ¬¡å»¶æ—¶={batch_delay}s, æ‰¹æ¬¡å¤§å°={batch_size}")
    
    def smart_delay(self, index: int = 0):
        """æ™ºèƒ½å»¶æ—¶æ§åˆ¶"""
        # åŸºç¡€å»¶æ—¶ + éšæœºå»¶æ—¶
        delay = self.base_delay + random.uniform(0, self.random_delay)
        time.sleep(delay)
        
        # æ¯æ‰¹æ¬¡åå¢åŠ é¢å¤–å»¶æ—¶
        if index > 0 and index % self.batch_size == 0:
            logger.info(f"æ‰¹æ¬¡å»¶æ—¶: {self.batch_delay}s")
            time.sleep(self.batch_delay)
    
    @retry(tries=3, delay=5)
    def safe_request(self, func, *args, **kwargs):
        """å®‰å…¨çš„APIè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"APIè¯·æ±‚å¤±è´¥: {e}")
            raise
    
    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†DataFrameæ•°æ®"""
        if df.empty:
            return df
        
        # æ›¿æ¢æ— ç©·å¤§å€¼
        df = df.replace([float('inf'), float('-inf')], None)
        
        # æ¸…ç†å­—ç¬¦ä¸²åˆ—çš„ç©ºæ ¼
        for col in df.select_dtypes(include=['object']).columns:
            df[col] = df[col].astype(str).str.strip()
            df[col] = df[col].replace(['nan', 'None', ''], None)
        
        return df


class StockBasicCollector(BaseCollector):
    """è‚¡ç¥¨åŸºç¡€ä¿¡æ¯é‡‡é›†å™¨"""
    
    def collect(self) -> bool:
        """é‡‡é›†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
        logger.info("å¼€å§‹é‡‡é›†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
        
        try:
            # è·å–æ›´å…¨é¢çš„Aè‚¡è‚¡ç¥¨ä¿¡æ¯ï¼ˆä¸œè´¢æ•°æ®æºï¼‰
            logger.info("æ­£åœ¨ä»ä¸œè´¢è·å–è‚¡ç¥¨åˆ—è¡¨...")
            stock_em = self.safe_request(ak.stock_zh_a_spot_em)
            
            if stock_em.empty:
                logger.warning("ä¸œè´¢è‚¡ç¥¨ä¿¡æ¯ä¸ºç©ºï¼Œå°è¯•å¤‡ç”¨æ•°æ®æº...")
                # å¤‡ç”¨æ•°æ®æº
                stock_basic = self.safe_request(ak.stock_info_a_code_name)
                if stock_basic.empty:
                    logger.error("æ‰€æœ‰è‚¡ç¥¨æ•°æ®æºéƒ½ä¸ºç©º")
                    return False
                
                # é‡å‘½ååˆ—
                stock_basic = stock_basic.rename(columns={
                    'code': 'stock_code',
                    'name': 'stock_name'
                })
                # æ·»åŠ ç¼ºå¤±å­—æ®µ
                stock_basic['exchange'] = stock_basic['stock_code'].apply(self._get_market_by_code)
                stock_basic['update_time'] = datetime.now()
                
            else:
                # ä½¿ç”¨ä¸œè´¢æ•°æ®ï¼Œæ˜ å°„åˆ°æ¨¡å‹å­—æ®µ
                logger.info(f"è·å–åˆ° {len(stock_em)} åªè‚¡ç¥¨çš„ä¿¡æ¯")
                
                # å­—æ®µæ˜ å°„
                column_mapping = {
                    'ä»£ç ': 'stock_code',
                    'åç§°': 'stock_name',
                    'æ€»å¸‚å€¼': 'total_share',  # ç”¨æ€»å¸‚å€¼è¿‘ä¼¼ä»£æ›¿æ€»è‚¡æœ¬
                    'æµé€šå¸‚å€¼': 'float_share',  # ç”¨æµé€šå¸‚å€¼è¿‘ä¼¼ä»£æ›¿æµé€šè‚¡æœ¬
                    'å¸‚ç›ˆç‡-åŠ¨æ€': 'pe_ratio',  # ä¸´æ—¶å­—æ®µï¼Œç”¨äºåˆ¤æ–­STç­‰
                    'å¸‚å‡€ç‡': 'pb_ratio'  # ä¸´æ—¶å­—æ®µ
                }
                
                # åˆ›å»ºåŸºç¡€ä¿¡æ¯DataFrame
                stock_basic = pd.DataFrame()
                for old_col, new_col in column_mapping.items():
                    if old_col in stock_em.columns:
                        stock_basic[new_col] = stock_em[old_col]
                
                # æ·»åŠ æ´¾ç”Ÿå­—æ®µ
                if 'stock_code' in stock_basic.columns:
                    # äº¤æ˜“æ‰€åˆ¤æ–­
                    stock_basic['exchange'] = stock_basic['stock_code'].apply(self._get_market_by_code)
                    
                    # STåˆ¤æ–­ï¼ˆé€šè¿‡è‚¡ç¥¨åç§°åˆ¤æ–­ï¼‰
                    if 'stock_name' in stock_basic.columns:
                        stock_basic['is_st'] = stock_basic['stock_name'].str.contains('ST|st', regex=True, na=False)
                    
                    # çŠ¶æ€å­—æ®µï¼ˆç®€å•è®¾ä¸ºæ­£å¸¸ï¼‰
                    stock_basic['status'] = 'æ­£å¸¸'
                
                # ç§»é™¤ä¸´æ—¶å­—æ®µ
                temp_fields = ['pe_ratio', 'pb_ratio']
                for field in temp_fields:
                    if field in stock_basic.columns:
                        stock_basic = stock_basic.drop(columns=[field])
                
                # æ·»åŠ æ›´æ–°æ—¶é—´
                stock_basic['update_time'] = datetime.now()
            
            # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…ç†
            if 'total_share' in stock_basic.columns:
                stock_basic['total_share'] = pd.to_numeric(stock_basic['total_share'], errors='coerce')
            if 'float_share' in stock_basic.columns:
                stock_basic['float_share'] = pd.to_numeric(stock_basic['float_share'], errors='coerce')
            
            # æ¸…ç†æ•°æ®
            stock_basic = self.clean_dataframe(stock_basic)
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            stock_basic = stock_basic.dropna(subset=['stock_code'])
            
            logger.info(f"å‡†å¤‡æ’å…¥ {len(stock_basic)} æ¡è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
            logger.info(f"æ•°æ®åˆ—: {stock_basic.columns.tolist()}")
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                stock_basic, 
                'stock_basic', 
                ['stock_code']
            )
            
            if success:
                logger.info(f"æˆåŠŸé‡‡é›†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ {len(stock_basic)} æ¡")
            
            return success
            
        except Exception as e:
            logger.error(f"é‡‡é›†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def _get_market_by_code(self, stock_code: str) -> str:
        """æ ¹æ®è‚¡ç¥¨ä»£ç åˆ¤æ–­æ‰€å±å¸‚åœº"""
        if not stock_code:
            return 'æœªçŸ¥'
        
        code = str(stock_code)
        if code.startswith('0') or code.startswith('2') or code.startswith('3'):
            return 'æ·±äº¤æ‰€'
        elif code.startswith('6') or code.startswith('9'):
            return 'ä¸Šäº¤æ‰€'
        elif code.startswith('8'):
            return 'åŒ—äº¤æ‰€'
        else:
            return 'å…¶ä»–'


class DailyQuoteCollector(BaseCollector):
    """æ—¥çº¿è¡Œæƒ…é‡‡é›†å™¨"""
    
    def collect_stock_history(self, stock_code: str, start_date: str, 
                            end_date: Optional[str] = None) -> bool:
        """é‡‡é›†å•ä¸ªè‚¡ç¥¨çš„å†å²æ•°æ®"""
        try:
            if end_date is None:
                end_date = datetime.now().strftime('%Y%m%d')
            
            logger.info(f"é‡‡é›†è‚¡ç¥¨ {stock_code} ä» {start_date} åˆ° {end_date} çš„è¡Œæƒ…æ•°æ®")
            
            # è·å–è‚¡ç¥¨å†å²æ•°æ®
            df = self.safe_request(
                ak.stock_zh_a_hist,
                symbol=stock_code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust=""
            )
            
            if df.empty:
                logger.warning(f"è‚¡ç¥¨ {stock_code} æ— å†å²æ•°æ®")
                return True
            
            # é‡å‘½ååˆ—
            column_mapping = {
                'æ—¥æœŸ': 'trade_date',
                'è‚¡ç¥¨ä»£ç ': 'stock_code',
                'å¼€ç›˜': 'open',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æ”¶ç›˜': 'close',
                'æ¶¨è·Œé¢': 'change',
                'æ¶¨è·Œå¹…': 'pct_chg',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount',
                'æŒ¯å¹…': 'amplitude',
                'æ¢æ‰‹ç‡': 'turnover_rate'
            }
            
            df = df.rename(columns=column_mapping)
            df['stock_code'] = stock_code
            df['update_time'] = datetime.now()
            
            # æ•°æ®ç±»å‹è½¬æ¢
            if 'trade_date' in df.columns:
                df['trade_date'] = pd.to_datetime(df['trade_date']).dt.date
            
            # æ¸…ç†æ•°æ®
            df = self.clean_dataframe(df)
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                df, 
                'daily_quote', 
                ['stock_code', 'trade_date']
            )
            
            if success:
                logger.info(f"æˆåŠŸé‡‡é›†è‚¡ç¥¨ {stock_code} è¡Œæƒ…æ•°æ® {len(df)} æ¡")
            
            # å»¶æ—¶æ§åˆ¶åœ¨è°ƒç”¨æ–¹å¤„ç†
            
            return success
            
        except Exception as e:
            logger.error(f"é‡‡é›†è‚¡ç¥¨ {stock_code} è¡Œæƒ…æ•°æ®å¤±è´¥: {e}")
            return False
    
    def collect_all_stocks_history(self, start_date: str, 
                                 end_date: Optional[str] = None, 
                                 enable_resume: bool = True) -> bool:
        """é‡‡é›†æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
        logger.info("å¼€å§‹é‡‡é›†æ‰€æœ‰è‚¡ç¥¨å†å²è¡Œæƒ…æ•°æ®...")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = db.get_stock_list()
        if not stock_list:
            logger.error("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return False
        
        success_count = 0
        skip_count = 0
        total_count = len(stock_list)
        logger.info(f"è‚¡ç¥¨åˆ—è¡¨æ•°é‡: {total_count}")
        for i, stock_code in enumerate(stock_list, 1):
            logger.info(f"å¤„ç†è¿›åº¦: {i}/{total_count} - {stock_code}")
            
            # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
            if enable_resume:
                latest_date = db.get_latest_date('daily_quote', 'trade_date', stock_code)
                if latest_date:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å®Œæ•´çš„æ•°æ®
                    if end_date and latest_date >= end_date:
                        logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        skip_count += 1
                        continue
                    # ä»æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹é‡‡é›†
                    resume_start_date = (datetime.strptime(latest_date, '%Y%m%d') + timedelta(days=1)).strftime('%Y%m%d')
                    if resume_start_date > start_date:
                        logger.info(f"è‚¡ç¥¨ {stock_code} ä» {resume_start_date} å¼€å§‹ç»­ä¼ ")
                        start_date_for_stock = resume_start_date
                    else:
                        start_date_for_stock = start_date
                else:
                    start_date_for_stock = start_date
            else:
                start_date_for_stock = start_date
            
            # é‡‡é›†æ•°æ®
            if self.collect_stock_history(stock_code, start_date_for_stock, end_date):
                success_count += 1
            
            # æ™ºèƒ½å»¶æ—¶æ§åˆ¶
            self.smart_delay(i)
            
            # æ¯100åªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡è¿›åº¦
            if i % 100 == 0:
                logger.info(f"å·²å¤„ç† {i}/{total_count} åªè‚¡ç¥¨ï¼ŒæˆåŠŸ {success_count} åªï¼Œè·³è¿‡ {skip_count} åª")
        
        logger.info(f"å†å²æ•°æ®é‡‡é›†å®Œæˆ: æ€»è®¡ {total_count} åªè‚¡ç¥¨ï¼ŒæˆåŠŸ {success_count} åªï¼Œè·³è¿‡ {skip_count} åª")
        return success_count > 0
    
    def collect_latest_quotes_batch(self) -> bool:
        """æ‰¹é‡é‡‡é›†æœ€æ–°è¡Œæƒ…æ•°æ®ï¼ˆé¿å…é¢‘ç¹APIè¯·æ±‚ï¼‰"""
        logger.info("å¼€å§‹æ‰¹é‡é‡‡é›†æœ€æ–°è¡Œæƒ…æ•°æ®...")
        
        try:
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ‰¹é‡æ¥å£è·å–æ‰€æœ‰è‚¡ç¥¨æœ€æ–°æ•°æ®
            logger.info("æ­£åœ¨è·å–æ‰€æœ‰è‚¡ç¥¨æœ€æ–°è¡Œæƒ…...")
            df = self.safe_request(ak.stock_zh_a_spot_em)
            
            if df.empty:
                logger.warning("æ‰¹é‡è¡Œæƒ…æ•°æ®ä¸ºç©º")
                return False
            
            logger.info(f"è·å–åˆ° {len(df)} åªè‚¡ç¥¨çš„æœ€æ–°è¡Œæƒ…")
            
            # åªé€‰æ‹©æ•°æ®åº“ä¸­å­˜åœ¨çš„æ ¸å¿ƒè¡Œæƒ…å­—æ®µ
            basic_columns = {
                'ä»£ç ': 'stock_code',
                'æœ€æ–°ä»·': 'close',
                'æ¶¨è·Œé¢': 'change',
                'æ¶¨è·Œå¹…': 'pct_chg',
                'ä»Šå¼€': 'open',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount'
            }
            
            # åˆ›å»ºæ–°çš„DataFrameï¼ŒåªåŒ…å«æ ¸å¿ƒè¡Œæƒ…å­—æ®µ
            processed_data = {}
            for old_col, new_col in basic_columns.items():
                if old_col in df.columns:
                    processed_data[new_col] = df[old_col]
                else:
                    processed_data[new_col] = None
            
            # åˆ›å»ºæ–°çš„DataFrame
            df_clean = pd.DataFrame(processed_data)
            
            # æ·»åŠ äº¤æ˜“æ—¥æœŸï¼ˆä½¿ç”¨å½“å‰æ—¥æœŸï¼‰
            today = datetime.now().date()
            df_clean['trade_date'] = today
            df_clean['update_time'] = datetime.now()
            
            # æ¸…ç†æ•°æ®
            df_clean = self.clean_dataframe(df_clean)
            
            # æ•°æ®ç±»å‹è½¬æ¢
            try:
                # è½¬æ¢æ•°å€¼ç±»å‹å­—æ®µ
                numeric_fields = ['close', 'change', 'pct_chg', 'open', 'high', 'low', 'amount']
                for field in numeric_fields:
                    if field in df_clean.columns:
                        df_clean[field] = pd.to_numeric(df_clean[field], errors='coerce')
                
                # æˆäº¤é‡è½¬æ¢ä¸ºæ•´æ•°
                if 'volume' in df_clean.columns:
                    df_clean['volume'] = pd.to_numeric(df_clean['volume'], errors='coerce')
                    df_clean['volume'] = df_clean['volume'].fillna(0).astype('int64')
                
            except Exception as e:
                logger.warning(f"æ•°æ®ç±»å‹è½¬æ¢æ—¶å‡ºç°è­¦å‘Š: {e}")
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            df_clean = df_clean.dropna(subset=['stock_code'])
            
            logger.info(f"å‡†å¤‡æ’å…¥ {len(df_clean)} æ¡æœ€æ–°è¡Œæƒ…æ•°æ®")
            logger.info(f"æ•°æ®åˆ—: {df_clean.columns.tolist()}")
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                df_clean, 
                'daily_quote', 
                ['stock_code', 'trade_date']
            )
            
            if success:
                logger.info(f"æˆåŠŸæ‰¹é‡é‡‡é›†æœ€æ–°è¡Œæƒ…æ•°æ® {len(df_clean)} æ¡")
                logger.info("ğŸš€ æ‰¹é‡é‡‡é›†é¿å…äº†æ•°åƒæ¬¡å•ç‹¬APIè¯·æ±‚ï¼")
            
            return success
            
        except Exception as e:
            logger.error(f"æ‰¹é‡é‡‡é›†æœ€æ–°è¡Œæƒ…æ•°æ®å¤±è´¥: {e}")
            return False

    def collect_latest_quotes(self) -> bool:
        """é‡‡é›†æœ€æ–°è¡Œæƒ…æ•°æ®"""
        logger.info("å¼€å§‹é‡‡é›†æœ€æ–°è¡Œæƒ…æ•°æ®...")
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶è¿›è¡Œæ‰¹é‡é‡‡é›†
        max_retries = 3
        retry_delay = 300  # 5åˆ†é’Ÿ = 300ç§’
        
        for attempt in range(max_retries):
            logger.info(f"æ‰¹é‡é‡‡é›†å°è¯• {attempt + 1}/{max_retries}")
            
            batch_success = self.collect_latest_quotes_batch()
            if batch_success:
                logger.info("æ‰¹é‡é‡‡é›†æˆåŠŸ")
                return True
            
            if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                logger.warning(f"æ‰¹é‡é‡‡é›†å¤±è´¥ï¼Œ{retry_delay//60}åˆ†é’Ÿåè¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                time.sleep(retry_delay)
            else:
                logger.error(f"æ‰¹é‡é‡‡é›†å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})ï¼Œæœ¬æ¬¡é‡‡é›†ç»“æŸ")
        
        return False


class IndexDataCollector(BaseCollector):
    """æŒ‡æ•°æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        super().__init__()
        self.index_list = [
            ('sh000001', 'ä¸Šè¯æŒ‡æ•°'),
            ('sz399001', 'æ·±è¯æˆæŒ‡'),
            ('sz399006', 'åˆ›ä¸šæ¿æŒ‡'),
            ('sh000300', 'æ²ªæ·±300'),
            ('sh000905', 'ä¸­è¯500')
        ]
    

    
    def collect_all_indexes_history(self, start_date: str, 
                                  end_date: Optional[str] = None,
                                  retry_delay_hours: int = 1) -> bool:
        """é‡‡é›†æ‰€æœ‰æŒ‡æ•°å†å²æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            retry_delay_hours: å½“æ•°æ®æºæœªæ›´æ–°æ—¶çš„é‡è¯•å»¶è¿Ÿå°æ—¶æ•°
        """
        logger.info("å¼€å§‹é‡‡é›†æŒ‡æ•°å†å²æ•°æ®...")
        
        success_count = 0
        has_data_count = 0  # å®é™…æœ‰æ•°æ®çš„æŒ‡æ•°æ•°é‡
        
        for i, (index_code, index_name) in enumerate(self.index_list, 1):
            result = self.collect_index_history_with_data_check(index_code, index_name, start_date, end_date)
            if result['success']:
                success_count += 1
            if result['has_data']:
                has_data_count += 1
            # æŒ‡æ•°æ•°æ®é‡‡é›†é—´éš”å»¶æ—¶
            if i < len(self.index_list):  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶æ—¶
                time.sleep(1.0)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŒ‡æ•°éƒ½æ²¡æœ‰æ•°æ®ï¼ˆå¯èƒ½æ•°æ®æºæœªæ›´æ–°ï¼‰
        if has_data_count == 0 and success_count == len(self.index_list):
            logger.warning(f"æ‰€æœ‰æŒ‡æ•°åœ¨ {start_date} éƒ½æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½æ•°æ®æºæœªæ›´æ–°")
            
            # å¦‚æœæ˜¯å½“æ—¥æ•°æ®ä¸”æ—¶é—´è¿˜æ—©ï¼Œå»ºè®®å»¶è¿Ÿé‡è¯•
            current_hour = datetime.now().hour
            if start_date == datetime.now().strftime('%Y%m%d') and current_hour < 20:
                logger.info(f"å»ºè®® {retry_delay_hours} å°æ—¶åé‡è¯•ï¼Œå½“å‰æ—¶é—´: {datetime.now().strftime('%H:%M')}")
                logger.info(f"å¯åœ¨ {(datetime.now() + timedelta(hours=retry_delay_hours)).strftime('%H:%M')} åé‡æ–°æ‰§è¡Œ")
                
                # å¯ä»¥é€‰æ‹©ç«‹å³é‡è¯•ä¸€æ¬¡ï¼ˆç­‰å¾…1å°æ—¶ï¼‰
                if self._should_retry_index_collection():
                    logger.info("å¯åŠ¨è‡ªåŠ¨é‡è¯•æœºåˆ¶...")
                    return self._retry_index_collection_later(start_date, end_date, retry_delay_hours)
        
        logger.info(f"æŒ‡æ•°å†å²æ•°æ®é‡‡é›†å®Œæˆ: æˆåŠŸ {success_count}/{len(self.index_list)} ä¸ªæŒ‡æ•°ï¼Œæœ‰æ•°æ® {has_data_count} ä¸ª")
        return success_count > 0

    def collect_index_history_with_data_check(self, index_code: str, index_name: str,
                                            start_date: str, end_date: Optional[str] = None) -> dict:
        """é‡‡é›†æŒ‡æ•°å†å²æ•°æ®å¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®"""
        try:
            if end_date is None:
                end_date = datetime.now().strftime('%Y%m%d')
            
            logger.info(f"é‡‡é›†æŒ‡æ•° {index_code}({index_name}) ä» {start_date} åˆ° {end_date} çš„æ•°æ®")
            
            # è·å–æŒ‡æ•°æ•°æ®
            df = self.safe_request(
                ak.stock_zh_index_daily,
                symbol=index_code
            )
            
            if df.empty:
                logger.warning(f"æŒ‡æ•° {index_code} æ— å†å²æ•°æ®")
                return {'success': True, 'has_data': False}
            
            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
            df['date'] = pd.to_datetime(df['date'])
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            df = df[(df['date'] >= start_dt) & (df['date'] <= end_dt)]
            
            if df.empty:
                logger.info(f"æŒ‡æ•° {index_code} åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ— æ•°æ®")
                return {'success': True, 'has_data': False}
            
            # é‡å‘½åå’Œæ·»åŠ åˆ—
            df = df.rename(columns={'date': 'trade_date'})
            df['index_code'] = index_code
            df['index_name'] = index_name
            df['trade_date'] = df['trade_date'].dt.date
            df['update_time'] = datetime.now()
            
            # è®¡ç®—æ¶¨è·Œé¢å’Œæ¶¨è·Œå¹…
            df = df.sort_values('trade_date')
            df['change'] = df['close'].diff()
            df['pct_chg'] = df['close'].pct_change() * 100
            
            # æ¸…ç†æ•°æ®
            df = self.clean_dataframe(df)
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                df,
                'index_data',
                ['index_code', 'trade_date']
            )
            
            if success:
                logger.info(f"æˆåŠŸé‡‡é›†æŒ‡æ•° {index_code} æ•°æ® {len(df)} æ¡")
            
            return {'success': success, 'has_data': True}
            
        except Exception as e:
            logger.error(f"é‡‡é›†æŒ‡æ•° {index_code} æ•°æ®å¤±è´¥: {e}")
            return {'success': False, 'has_data': False}

    def _should_retry_index_collection(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•æŒ‡æ•°æ•°æ®é‡‡é›†"""
        current_time = datetime.now()
        current_hour = current_time.hour
        
        # åªåœ¨äº¤æ˜“æ—¥çš„ç‰¹å®šæ—¶é—´æ®µå†…é‡è¯•
        if current_time.weekday() >= 5:  # å‘¨æœ«ä¸é‡è¯•
            return False
        
        # åœ¨18:00-21:00ä¹‹é—´å¯ä»¥é‡è¯•
        if 18 <= current_hour <= 21:
            return True
        
        return False

    def _retry_index_collection_later(self, start_date: str, end_date: Optional[str], 
                                    delay_hours: int) -> bool:
        """å»¶è¿Ÿé‡è¯•æŒ‡æ•°æ•°æ®é‡‡é›†"""
        import threading
        import time as time_module
        
        def delayed_retry():
            logger.info(f"ç­‰å¾… {delay_hours} å°æ—¶åé‡è¯•æŒ‡æ•°æ•°æ®é‡‡é›†...")
            time_module.sleep(delay_hours * 3600)  # è½¬æ¢ä¸ºç§’
            
            logger.info("å¼€å§‹é‡è¯•æŒ‡æ•°æ•°æ®é‡‡é›†...")
            retry_result = self.collect_all_indexes_history(start_date, end_date, retry_delay_hours=0)
            
            if retry_result:
                logger.info("é‡è¯•æˆåŠŸï¼šæŒ‡æ•°æ•°æ®é‡‡é›†å®Œæˆ")
                # å‘é€æˆåŠŸé€šçŸ¥
                try:
                    from feishu_notify import send_completion_notice
                    send_completion_notice(
                        "æŒ‡æ•°æ•°æ®é‡è¯•é‡‡é›†",
                        True,
                        {
                            "é‡è¯•æ—¶é—´": datetime.now().strftime('%H:%M:%S'),
                            "é‡‡é›†æ—¥æœŸ": start_date,
                            "çŠ¶æ€": "âœ… é‡è¯•æˆåŠŸ"
                        }
                    )
                except:
                    pass
            else:
                logger.warning("é‡è¯•å¤±è´¥ï¼šæŒ‡æ•°æ•°æ®ä»ç„¶æ— æ³•è·å–")
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå»¶è¿Ÿé‡è¯•
        retry_thread = threading.Thread(target=delayed_retry, daemon=True)
        retry_thread.start()
        
        logger.info(f"å·²å¯åŠ¨åå°é‡è¯•ä»»åŠ¡ï¼Œå°†åœ¨ {delay_hours} å°æ—¶åè‡ªåŠ¨é‡è¯•")
        return True  # è¿”å›Trueè¡¨ç¤ºå·²å®‰æ’é‡è¯•


class TradeCalendarCollector(BaseCollector):
    """äº¤æ˜“æ—¥å†é‡‡é›†å™¨"""
    
    def collect(self, start_year: Optional[int] = None, end_year: Optional[int] = None) -> bool:
        """é‡‡é›†å®Œæ•´äº¤æ˜“æ—¥å†ï¼ˆåªåŒºåˆ†äº¤æ˜“æ—¥å’Œéäº¤æ˜“æ—¥ï¼‰"""
        logger.info("å¼€å§‹é‡‡é›†å®Œæ•´äº¤æ˜“æ—¥å†...")
        
        try:
            if start_year is None:
                start_year = datetime.now().year - 2
            if end_year is None:
                end_year = datetime.now().year + 1
            
            logger.info(f"é‡‡é›†å¹´ä»½èŒƒå›´: {start_year} - {end_year}")
            
            # 1. è·å–äº¤æ˜“æ—¥æ•°æ®
            trade_dates_df = self.safe_request(ak.tool_trade_date_hist_sina)
            
            if trade_dates_df.empty:
                logger.warning("äº¤æ˜“æ—¥å†æ•°æ®ä¸ºç©º")
                return False
            
            # è½¬æ¢äº¤æ˜“æ—¥æœŸæ ¼å¼å¹¶åˆ›å»ºé›†åˆç”¨äºå¿«é€ŸæŸ¥æ‰¾
            trade_dates_df['trade_date'] = pd.to_datetime(trade_dates_df['trade_date'])
            trade_dates_set = set(trade_dates_df['trade_date'].dt.date)
            
            logger.info(f"è·å–åˆ° {len(trade_dates_set)} ä¸ªäº¤æ˜“æ—¥")
            
            # 2. ç”Ÿæˆå®Œæ•´æ—¥å†èŒƒå›´
            start_date = datetime(start_year, 1, 1).date()
            end_date = datetime(end_year, 12, 31).date()
            
            calendar_data = []
            current_date = start_date
            
            logger.info(f"ç”Ÿæˆæ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
            
            while current_date <= end_date:
                # åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
                is_trade_day = current_date in trade_dates_set
                
                calendar_data.append({
                    'calendar_date': current_date,
                    'is_trade_day': is_trade_day,
                    'update_time': datetime.now()
                })
                
                current_date += timedelta(days=1)
            
            # 3. è½¬æ¢ä¸ºDataFrame
            calendar_df = pd.DataFrame(calendar_data)
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_days = len(calendar_df)
            trade_days = sum(calendar_df['is_trade_day'])
            non_trade_days = total_days - trade_days
            
            logger.info(f"ç”Ÿæˆå®Œæ•´æ—¥å†æ•°æ® {total_days} æ¡")
            logger.info(f"å…¶ä¸­äº¤æ˜“æ—¥ {trade_days} æ¡")
            logger.info(f"éäº¤æ˜“æ—¥ {non_trade_days} æ¡")
            
            # 4. æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                calendar_df,
                'trade_calendar',
                ['calendar_date']
            )
            
            if success:
                logger.info(f"âœ… æˆåŠŸé‡‡é›†å®Œæ•´äº¤æ˜“æ—¥å† {len(calendar_df)} æ¡")
                
                # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹æ•°æ®
                logger.info("ç¤ºä¾‹æ•°æ®é¢„è§ˆ:")
                sample_data = calendar_df.head(10)
                for _, row in sample_data.iterrows():
                    status = "äº¤æ˜“æ—¥" if bool(row['is_trade_day']) else "éäº¤æ˜“æ—¥"
                    logger.info(f"  {row['calendar_date']} - {status}")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ é‡‡é›†äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return False


class StockHotRankCollector(BaseCollector):
    """è‚¡ç¥¨äººæ°”æ¦œé‡‡é›†å™¨"""
    
    def collect_hot_rank(self) -> bool:
        """é‡‡é›†è‚¡ç¥¨äººæ°”æ¦œæ•°æ®"""
        logger.info("å¼€å§‹é‡‡é›†è‚¡ç¥¨äººæ°”æ¦œæ•°æ®...")
        
        try:
            # è·å–äººæ°”æ¦œæ•°æ®
            df = self.safe_request(ak.stock_hot_rank_em)
            
            if df.empty:
                logger.warning("äººæ°”æ¦œæ•°æ®ä¸ºç©º")
                return False
            
            logger.info(f"è·å–åˆ° {len(df)} æ¡äººæ°”æ¦œæ•°æ®")
            
            # å­—æ®µæ˜ å°„
            column_mapping = {
                'å½“å‰æ’å': 'current_rank',
                'ä»£ç ': 'stock_code',
                'è‚¡ç¥¨åç§°': 'stock_name',
                'æœ€æ–°ä»·': 'latest_price',
                'æ¶¨è·Œé¢': 'change',
                'æ¶¨è·Œå¹…': 'pct_chg'
            }
            
            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)
            
            # æ·»åŠ äº¤æ˜“æ—¥æœŸå’Œæ›´æ–°æ—¶é—´
            today = datetime.now().date()
            df['trade_date'] = today
            df['update_time'] = datetime.now()
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_fields = ['current_rank', 'latest_price', 'change', 'pct_chg']
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors='coerce')
            
            # æ¸…ç†æ•°æ®
            df = self.clean_dataframe(df)
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            df = df.dropna(subset=['stock_code', 'current_rank'])
            
            logger.info(f"å‡†å¤‡æ’å…¥ {len(df)} æ¡äººæ°”æ¦œæ•°æ®")
            logger.info(f"æ•°æ®åˆ—: {df.columns.tolist()}")
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                df,
                'stock_hot_rank',
                ['trade_date', 'current_rank']
            )
            
            if success:
                logger.info(f"âœ… æˆåŠŸé‡‡é›†è‚¡ç¥¨äººæ°”æ¦œæ•°æ® {len(df)} æ¡")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ é‡‡é›†è‚¡ç¥¨äººæ°”æ¦œæ•°æ®å¤±è´¥: {e}")
            return False


class StockHotUpCollector(BaseCollector):
    """è‚¡ç¥¨é£™å‡æ¦œé‡‡é›†å™¨"""
    
    def collect_hot_up(self) -> bool:
        """é‡‡é›†è‚¡ç¥¨é£™å‡æ¦œæ•°æ®"""
        logger.info("å¼€å§‹é‡‡é›†è‚¡ç¥¨é£™å‡æ¦œæ•°æ®...")
        
        try:
            # è·å–é£™å‡æ¦œæ•°æ®
            df = self.safe_request(ak.stock_hot_up_em)
            
            if df.empty:
                logger.warning("é£™å‡æ¦œæ•°æ®ä¸ºç©º")
                return False
            
            logger.info(f"è·å–åˆ° {len(df)} æ¡é£™å‡æ¦œæ•°æ®")
            
            # å­—æ®µæ˜ å°„
            column_mapping = {
                'æ’åè¾ƒæ˜¨æ—¥å˜åŠ¨': 'rank_change',
                'å½“å‰æ’å': 'current_rank',
                'ä»£ç ': 'stock_code',
                'è‚¡ç¥¨åç§°': 'stock_name',
                'æœ€æ–°ä»·': 'latest_price',
                'æ¶¨è·Œé¢': 'change',
                'æ¶¨è·Œå¹…': 'pct_chg'
            }
            
            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)
            
            # æ·»åŠ äº¤æ˜“æ—¥æœŸå’Œæ›´æ–°æ—¶é—´
            today = datetime.now().date()
            df['trade_date'] = today
            df['update_time'] = datetime.now()
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_fields = ['rank_change', 'current_rank', 'latest_price', 'change', 'pct_chg']
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors='coerce')
            
            # æ¸…ç†æ•°æ®
            df = self.clean_dataframe(df)
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            df = df.dropna(subset=['stock_code', 'current_rank'])
            
            logger.info(f"å‡†å¤‡æ’å…¥ {len(df)} æ¡é£™å‡æ¦œæ•°æ®")
            logger.info(f"æ•°æ®åˆ—: {df.columns.tolist()}")
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                df,
                'stock_hot_up',
                ['trade_date', 'current_rank']
            )
            
            if success:
                logger.info(f"âœ… æˆåŠŸé‡‡é›†è‚¡ç¥¨é£™å‡æ¦œæ•°æ® {len(df)} æ¡")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ é‡‡é›†è‚¡ç¥¨é£™å‡æ¦œæ•°æ®å¤±è´¥: {e}")
            return False


class HsgtFundFlowCollector(BaseCollector):
    """æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘é‡‡é›†å™¨"""
    
    def collect_hsgt_fund_flow(self) -> bool:
        """é‡‡é›†æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®"""
        logger.info("å¼€å§‹é‡‡é›†æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®...")
        
        try:
            # è·å–æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®
            df = self.safe_request(ak.stock_hsgt_fund_flow_summary_em)
            
            if df.empty:
                logger.warning("æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®ä¸ºç©º")
                return False
            
            logger.info(f"è·å–åˆ° {len(df)} æ¡æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®")
            
            # å­—æ®µæ˜ å°„
            column_mapping = {
                'äº¤æ˜“æ—¥': 'trade_date',
                'ç±»å‹': 'type',
                'æ¿å—': 'sector',
                'èµ„é‡‘æ–¹å‘': 'direction',
                'äº¤æ˜“çŠ¶æ€': 'trade_status',
                'æˆäº¤å‡€ä¹°é¢': 'net_buy_amount',
                'èµ„é‡‘å‡€æµå…¥': 'net_inflow',
                'å½“æ—¥èµ„é‡‘ä½™é¢': 'day_balance',
                'ä¸Šæ¶¨æ•°': 'up_count',
                'æŒå¹³æ•°': 'flat_count',
                'ä¸‹è·Œæ•°': 'down_count',
                'ç›¸å…³æŒ‡æ•°': 'related_index',
                'æŒ‡æ•°æ¶¨è·Œå¹…': 'index_pct_chg'
            }
            
            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)
            
            # æ•°æ®ç±»å‹è½¬æ¢
            if 'trade_date' in df.columns:
                df['trade_date'] = pd.to_datetime(df['trade_date']).dt.date
            
            # æ•°å€¼å­—æ®µè½¬æ¢
            numeric_fields = ['trade_status', 'net_buy_amount', 'net_inflow', 'day_balance', 
                            'up_count', 'flat_count', 'down_count', 'index_pct_chg']
            for field in numeric_fields:
                if field in df.columns:
                    df[field] = pd.to_numeric(df[field], errors='coerce')
            
            # æ·»åŠ æ›´æ–°æ—¶é—´
            df['update_time'] = datetime.now()
            
            # æ¸…ç†æ•°æ®
            df = self.clean_dataframe(df)
            
            # ç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
            df = df.dropna(subset=['trade_date', 'type', 'sector', 'direction'])
            
            logger.info(f"å‡†å¤‡æ’å…¥ {len(df)} æ¡æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®")
            logger.info(f"æ•°æ®åˆ—: {df.columns.tolist()}")
            
            # æ’å…¥æ•°æ®åº“
            success = db.upsert_dataframe(
                df,
                'hsgt_fund_flow',
                ['trade_date', 'type', 'sector', 'direction']
            )
            
            if success:
                logger.info(f"âœ… æˆåŠŸé‡‡é›†æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ® {len(df)} æ¡")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ é‡‡é›†æ²ªæ·±æ¸¯é€šèµ„é‡‘æµå‘æ•°æ®å¤±è´¥: {e}")
            return False


class StockFundFlowRankCollector(BaseCollector):
    """ä¸ªè‚¡èµ„é‡‘æµæ’åé‡‡é›†å™¨"""
    
    def collect_fund_flow_rank(self, indicators: Optional[list] = None) -> bool:
        """é‡‡é›†ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®"""
        if indicators is None:
            indicators = ["ä»Šæ—¥", "3æ—¥", "5æ—¥", "10æ—¥"]
        
        logger.info("å¼€å§‹é‡‡é›†ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®...")
        
        overall_success = True
        today = datetime.now().date()
        
        for indicator in indicators:
            logger.info(f"é‡‡é›† {indicator} èµ„é‡‘æµæ’åæ•°æ®...")
            
            try:
                # è·å–æŒ‡å®šå‘¨æœŸçš„ä¸ªè‚¡èµ„é‡‘æµæ’å
                df = self.safe_request(ak.stock_individual_fund_flow_rank, indicator=indicator)
                
                if df.empty:
                    logger.warning(f"{indicator} ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®ä¸ºç©º")
                    overall_success = False
                    continue
                
                logger.info(f"è·å–åˆ° {len(df)} æ¡ {indicator} ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®")
                
                # å­—æ®µæ˜ å°„ - æ ¹æ®ä¸åŒå‘¨æœŸåŠ¨æ€è°ƒæ•´å­—æ®µå
                column_mapping = {
                    'åºå·': 'rank',
                    'ä»£ç ': 'stock_code',
                    'åç§°': 'stock_name',
                    'æœ€æ–°ä»·': 'latest_price'
                }
                
                # æ ¹æ®ä¸åŒå‘¨æœŸè®¾ç½®ç›¸åº”çš„å­—æ®µæ˜ å°„
                period_prefix = indicator if indicator != "ä»Šæ—¥" else "ä»Šæ—¥"
                
                # æ¶¨è·Œå¹…å­—æ®µ
                if f'{period_prefix}æ¶¨è·Œå¹…' in df.columns:
                    column_mapping[f'{period_prefix}æ¶¨è·Œå¹…'] = 'pct_chg'
                elif 'ä»Šæ—¥æ¶¨è·Œå¹…' in df.columns:
                    column_mapping['ä»Šæ—¥æ¶¨è·Œå¹…'] = 'pct_chg'
                
                # èµ„é‡‘æµå‘å­—æ®µæ˜ å°„
                fund_flow_mappings = [
                    (f'{period_prefix}ä¸»åŠ›å‡€æµå…¥-å‡€é¢', 'main_net_inflow_amount'),
                    (f'{period_prefix}ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”', 'main_net_inflow_rate'),
                    (f'{period_prefix}è¶…å¤§å•å‡€æµå…¥-å‡€é¢', 'super_large_net_amount'),
                    (f'{period_prefix}è¶…å¤§å•å‡€æµå…¥-å‡€å æ¯”', 'super_large_net_rate'),
                    (f'{period_prefix}å¤§å•å‡€æµå…¥-å‡€é¢', 'large_net_amount'),
                    (f'{period_prefix}å¤§å•å‡€æµå…¥-å‡€å æ¯”', 'large_net_rate'),
                    (f'{period_prefix}ä¸­å•å‡€æµå…¥-å‡€é¢', 'medium_net_amount'),
                    (f'{period_prefix}ä¸­å•å‡€æµå…¥-å‡€å æ¯”', 'medium_net_rate'),
                    (f'{period_prefix}å°å•å‡€æµå…¥-å‡€é¢', 'small_net_amount'),
                    (f'{period_prefix}å°å•å‡€æµå…¥-å‡€å æ¯”', 'small_net_rate')
                ]
                
                for old_col, new_col in fund_flow_mappings:
                    if old_col in df.columns:
                        column_mapping[old_col] = new_col
                
                # é‡å‘½ååˆ—
                df = df.rename(columns=column_mapping)
                
                # æ·»åŠ å‘¨æœŸæŒ‡æ ‡å’Œäº¤æ˜“æ—¥æœŸ
                df['indicator'] = indicator
                df['trade_date'] = today
                df['update_time'] = datetime.now()
                
                # æ•°æ®ç±»å‹è½¬æ¢
                numeric_fields = ['rank', 'latest_price', 'pct_chg', 'main_net_inflow_amount', 
                                'main_net_inflow_rate', 'super_large_net_amount', 'super_large_net_rate',
                                'large_net_amount', 'large_net_rate', 'medium_net_amount', 'medium_net_rate',
                                'small_net_amount', 'small_net_rate']
                
                for field in numeric_fields:
                    if field in df.columns:
                        df[field] = pd.to_numeric(df[field], errors='coerce')
                
                # æ¸…ç†æ•°æ®
                df = self.clean_dataframe(df)
                
                # ç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                df = df.dropna(subset=['stock_code', 'stock_name'])
                
                # è¿‡æ»¤æ‰åœ¨stock_basicè¡¨ä¸­ä¸å­˜åœ¨çš„è‚¡ç¥¨ä»£ç ï¼Œé¿å…å¤–é”®çº¦æŸé”™è¯¯
                from database import db
                valid_stocks = db.get_stock_list()
                before_count = len(df)
                df_filtered = df[df['stock_code'].isin(valid_stocks)].copy()
                after_count = len(df_filtered)
                
                if before_count > after_count:
                    logger.info(f"è¿‡æ»¤æ‰ {before_count - after_count} åªä¸åœ¨åŸºç¡€è¡¨ä¸­çš„è‚¡ç¥¨")
                    logger.info(f"å‰©ä½™æœ‰æ•ˆè‚¡ç¥¨æ•°æ®: {after_count} æ¡")
                
                df = pd.DataFrame(df_filtered)
                
                # æ•°æ®å»é‡ - æŒ‰å”¯ä¸€çº¦æŸå­—æ®µå»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¡è®°å½•
                unique_fields = ['stock_code', 'indicator', 'trade_date']
                before_dedup_count = len(df)
                df = df.drop_duplicates(subset=unique_fields, keep='first')
                after_dedup_count = len(df)
                
                if before_dedup_count > after_dedup_count:
                    logger.info(f"æ•°æ®å»é‡: åˆ é™¤ {before_dedup_count - after_dedup_count} æ¡é‡å¤è®°å½•")
                    logger.info(f"å»é‡åå‰©ä½™æ•°æ®: {after_dedup_count} æ¡")
                
                logger.info(f"å‡†å¤‡æ’å…¥ {len(df)} æ¡ {indicator} ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®")
                logger.info(f"æ•°æ®åˆ—: {df.columns.tolist()}")
                
                # æ’å…¥æ•°æ®åº“
                success = db.upsert_dataframe(
                    df,
                    'stock_fund_flow_rank',
                    ['stock_code', 'indicator', 'trade_date']
                )
                
                if success:
                    logger.info(f"âœ… æˆåŠŸé‡‡é›† {indicator} ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ® {len(df)} æ¡")
                else:
                    overall_success = False
                    logger.error(f"âŒ {indicator} ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®æ’å…¥å¤±è´¥")
                
                # å»¶æ—¶æ§åˆ¶
                self.smart_delay()
                
            except Exception as e:
                logger.error(f"âŒ é‡‡é›† {indicator} ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®å¤±è´¥: {e}")
                overall_success = False
        
        if overall_success:
            logger.info("âœ… ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®é‡‡é›†å®Œæˆ")
        else:
            logger.warning("âš ï¸ ä¸ªè‚¡èµ„é‡‘æµæ’åæ•°æ®éƒ¨åˆ†é‡‡é›†å¤±è´¥")
        
        return overall_success


class StockNewsCollector(BaseCollector):
    """ä¸œæ–¹è´¢å¯Œå…¨çƒè´¢ç»å¿«è®¯é‡‡é›†å™¨"""
    
    def __init__(self):
        super().__init__()
        # è®¾ç½®é€‚åˆæ–°é—»é‡‡é›†çš„å»¶æ—¶é…ç½®
        self.set_delay_config(
            base_delay=0.1,      # åŸºç¡€å»¶æ—¶0.1ç§’ï¼ˆæ›´å¿«ï¼‰
            random_delay=0.2,    # éšæœºå»¶æ—¶æœ€å¤š0.2ç§’
            batch_delay=2.0,     # æ‰¹æ¬¡å»¶æ—¶2ç§’
            batch_size=50        # æ¯50æ¡å¤„ç†ä¸€æ‰¹
        )
    
    def collect_news(self, max_process_count: int = 10) -> bool:
        """é‡‡é›†ä¸œæ–¹è´¢å¯Œå…¨çƒè´¢ç»å¿«è®¯"""
        logger.info("å¼€å§‹é‡‡é›†ä¸œæ–¹è´¢å¯Œå…¨çƒè´¢ç»å¿«è®¯...")
        
        try:
            # ä½¿ç”¨ akshare è·å–ä¸œæ–¹è´¢å¯Œå…¨çƒè´¢ç»å¿«è®¯æ•°æ®
            logger.info("æ­£åœ¨ä»ä¸œæ–¹è´¢å¯Œè·å–å…¨çƒè´¢ç»å¿«è®¯...")
            news_df = self.safe_request(ak.stock_info_global_em)
            
            if news_df.empty:
                logger.warning("ä¸œæ–¹è´¢å¯Œæ–°é—»æ•°æ®ä¸ºç©º")
                return False
            
            original_count = len(news_df)
            logger.info(f"è·å–åˆ° {original_count} æ¡æ–°é—»æ•°æ®")
            
            # é™åˆ¶å¤„ç†æ•°é‡ - åªå¤„ç†æœ€æ–°çš„æ•°æ®
            if original_count > max_process_count:
                news_df = news_df.head(max_process_count)
                logger.info(f"é™åˆ¶å¤„ç†æ•°é‡ï¼Œåªå¤„ç†æœ€æ–°çš„ {max_process_count} æ¡æ–°é—»")
            
            # æ•°æ®æ¸…ç†å’Œè½¬æ¢
            news_df = self.clean_dataframe(news_df)
            
            # å­—æ®µæ˜ å°„å’Œå¤„ç†
            processed_news = []
            new_news_count = 0
            duplicate_count = 0
            processed_urls = set()  # è®°å½•å·²å¤„ç†çš„URLï¼Œé¿å…æ‰¹æ¬¡å†…é‡å¤
            
            for idx, (index, row) in enumerate(news_df.iterrows()):
                try:
                    # æå–åŸºç¡€æ•°æ®ï¼ˆæ–°æ¥å£å­—æ®µï¼šæ ‡é¢˜ã€æ‘˜è¦ã€å‘å¸ƒæ—¶é—´ã€é“¾æ¥ï¼‰
                    url = str(row.get('é“¾æ¥', '')).strip()
                    if not url:
                        continue
                    
                    # æ£€æŸ¥æ‰¹æ¬¡å†…æ˜¯å¦é‡å¤
                    if url in processed_urls:
                        duplicate_count += 1
                        continue
                    
                    # å¿«é€Ÿæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨
                    try:
                        existing_check = db.supabase.table('stock_news').select('id').eq('url', url).limit(1).execute()
                        if existing_check.data:
                            duplicate_count += 1
                            # æ¯100ä¸ªé‡å¤æ–°é—»è¾“å‡ºä¸€æ¬¡è¿›åº¦
                            if duplicate_count % 100 == 0:
                                logger.info(f"å·²è·³è¿‡ {duplicate_count} æ¡é‡å¤æ–°é—»...")
                            continue
                    except:
                        pass  # æ£€æŸ¥å¤±è´¥å°±ç»§ç»­å¤„ç†
                    
                    # è®°å½•URLåˆ°å·²å¤„ç†é›†åˆ
                    processed_urls.add(url)
                    
                    # å‡†å¤‡æ•°æ®ï¼ˆå­—æ®µæ˜ å°„ï¼šæ ‡é¢˜->tag, æ‘˜è¦->summary, å‘å¸ƒæ—¶é—´->pub_timeç­‰ï¼‰
                    title_value = row.get('æ ‡é¢˜')
                    summary_value = row.get('æ‘˜è¦')
                    pub_time_value = row.get('å‘å¸ƒæ—¶é—´')
                    
                    news_data = {
                        'url': url,
                        'tag': str(title_value)[:100] if title_value is not None and not pd.isna(title_value) else None,
                        'summary': str(summary_value) if summary_value is not None and not pd.isna(summary_value) else None,
                        'pub_time': str(pub_time_value)[:50] if pub_time_value is not None and not pd.isna(pub_time_value) else None,
                        'pub_date_time': self._parse_pub_time(pub_time_value),
                        'create_time': datetime.now(),
                        'update_time': datetime.now()
                    }
                    
                    processed_news.append(news_data)
                    new_news_count += 1
                    
                    # æ¯50æ¡æ–°æ–°é—»è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if new_news_count % 50 == 0:
                        logger.info(f"å·²å¤„ç† {new_news_count} æ¡æ–°æ–°é—»ï¼Œè·³è¿‡ {duplicate_count} æ¡é‡å¤æ–°é—»")
                    
                    # å¦‚æœè¿ç»­100æ¡éƒ½æ˜¯é‡å¤çš„ï¼Œæå‰ç»“æŸ
                    if duplicate_count > 100 and new_news_count == 0:
                        logger.info("è¿ç»­å‘ç°å¤§é‡é‡å¤æ–°é—»ï¼Œæå‰ç»“æŸé‡‡é›†")
                        break
                    
                    # æ™ºèƒ½å»¶æ—¶æ§åˆ¶ - å‡å°‘å»¶æ—¶é¢‘ç‡
                    if idx % 10 == 0:  # æ¯10æ¡å»¶æ—¶ä¸€æ¬¡
                        self.smart_delay(idx // 10)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†ç¬¬ {idx} æ¡æ–°é—»å¤±è´¥: {e}")
                    continue
            
            if not processed_news:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–°é—»æ•°æ®")
                return False
            
            # è½¬æ¢ä¸ºDataFrame
            news_insert_df = pd.DataFrame(processed_news)
            
            logger.info(f"å‡†å¤‡æ’å…¥ {len(news_insert_df)} æ¡æ–°é—»æ•°æ®")
            
            # æ’å…¥æ•°æ®åº“ï¼ˆåŸºäºURLå»é‡ï¼‰
            success = db.upsert_dataframe(
                news_insert_df,
                'stock_news',
                ['url']  # ä½¿ç”¨URLä½œä¸ºå”¯ä¸€æ ‡è¯†
            )
            
            if success:
                logger.info(f"æˆåŠŸé‡‡é›†ä¸œæ–¹è´¢å¯Œå…¨çƒè´¢ç»å¿«è®¯ {len(news_insert_df)} æ¡")
                
                # æ¸…ç†è¶…è¿‡ä¸€å‘¨çš„æ—§æ–°é—»
                self._cleanup_old_news()
            
            return success
            
        except Exception as e:
            logger.error(f"é‡‡é›†ä¸œæ–¹è´¢å¯Œå…¨çƒè´¢ç»å¿«è®¯å¤±è´¥: {e}")
            return False
    
    def _parse_pub_time(self, pub_time_str) -> Optional[datetime]:
        """è§£æå‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        if not pub_time_str or pd.isna(pub_time_str):
            return None
            
        try:
            import re
            pub_time_str = str(pub_time_str).strip()
            
            # æ ¼å¼1: YYYY-MM-DD HH:MM:SS.mmm (å¸¦æ¯«ç§’)
            if re.match(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+', pub_time_str):
                # ç§»é™¤æ¯«ç§’éƒ¨åˆ†
                base_time = re.sub(r'\.\d+$', '', pub_time_str)
                return datetime.strptime(base_time, '%Y-%m-%d %H:%M:%S')
            
            # æ ¼å¼2: YYYY-MM-DD HH:MM:SS (ä¸å¸¦æ¯«ç§’)
            elif re.match(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}$', pub_time_str):
                return datetime.strptime(pub_time_str, '%Y-%m-%d %H:%M:%S')
            
            # æ ¼å¼3: YYYY-MM-DD
            elif re.match(r'\d{4}-\d{2}-\d{2}$', pub_time_str):
                return datetime.strptime(pub_time_str, '%Y-%m-%d')
            
            # æ ¼å¼4: MMæœˆDDæ—¥
            elif 'æœˆ' in pub_time_str and 'æ—¥' in pub_time_str:
                month_match = re.search(r'(\d+)æœˆ', pub_time_str)
                day_match = re.search(r'(\d+)æ—¥', pub_time_str)
                if month_match and day_match:
                    current_year = datetime.now().year
                    month = int(month_match.group(1))
                    day = int(day_match.group(1))
                    return datetime(current_year, month, day)
            
            return None
            
        except Exception as e:
            logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {pub_time_str}, é”™è¯¯: {e}")
            return None
    
    def _cleanup_old_news(self):
        """æ¸…ç†è¶…è¿‡ä¸€å‘¨çš„æ—§æ–°é—»"""
        try:
            one_week_ago = datetime.now() - timedelta(days=7)
            one_week_ago_str = one_week_ago.isoformat()
            
            # æŸ¥è¯¢è¦åˆ é™¤çš„è®°å½•æ•°é‡
            old_news = db.supabase.table('stock_news').select('id').lt('create_time', one_week_ago_str).execute()
            delete_count = len(old_news.data) if old_news.data else 0
            
            if delete_count > 0:
                # æ‰§è¡Œåˆ é™¤
                db.supabase.table('stock_news').delete().lt('create_time', one_week_ago_str).execute()
                logger.info(f"å·²æ¸…ç† {delete_count} æ¡è¶…è¿‡ä¸€å‘¨çš„æ—§æ–°é—»")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ–°é—»å¤±è´¥: {e}")
    
    def get_latest_news(self, limit: int = 50, tag_filter: Optional[str] = None) -> List[Dict]:
        """è·å–æœ€æ–°æ–°é—»æ•°æ®"""
        try:
            query_builder = db.supabase.table('stock_news').select('*')
            
            if tag_filter:
                query_builder = query_builder.ilike('tag', f"%{tag_filter}%")
            
            result = query_builder.order('create_time', desc=True).limit(limit).execute()
            
            return result.data if result.data else []
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°æ–°é—»å¤±è´¥: {e}")
            return []
    
    def search_news(self, keyword: str, limit: int = 50) -> List[Dict]:
        """æœç´¢æ–°é—»"""
        try:
            result = db.supabase.table('stock_news').select('*').or_(
                f"tag.ilike.%{keyword}%,summary.ilike.%{keyword}%"
            ).order('create_time', desc=True).limit(limit).execute()
            
            return result.data if result.data else []
            
        except Exception as e:
            logger.error(f"æœç´¢æ–°é—»å¤±è´¥: {e}")
            return []
    
    def get_news_stats(self) -> Dict:
        """è·å–æ–°é—»ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # æ€»æ–°é—»æ•°
            total_result = db.count_records('stock_news')
            
            # ä»Šæ—¥æ–°å¢æ–°é—»æ•°
            today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            today_start_str = today_start.isoformat()
            today_result = db.supabase.table('stock_news').select('id').gte('create_time', today_start_str).execute()
            today_count = len(today_result.data) if today_result.data else 0
            
            # æœ¬å‘¨æ–°å¢æ–°é—»æ•°
            week_start = today_start - timedelta(days=today_start.weekday())
            week_start_str = week_start.isoformat()
            week_result = db.supabase.table('stock_news').select('id').gte('create_time', week_start_str).execute()
            week_count = len(week_result.data) if week_result.data else 0
            
            # æœ€æ–°æ–°é—»æ—¶é—´
            latest_result = db.supabase.table('stock_news').select('create_time').order('create_time', desc=True).limit(1).execute()
            latest_time = latest_result.data[0]['create_time'] if latest_result.data else None
            
            # çƒ­é—¨æ ‡ç­¾ç»Ÿè®¡ï¼ˆæœ€è¿‘ä¸€å‘¨ï¼‰
            week_ago = datetime.now() - timedelta(days=7)
            week_ago_str = week_ago.isoformat()
            tag_result = db.supabase.table('stock_news').select('tag').gte('create_time', week_ago_str).execute()
            
            # ç»Ÿè®¡æ ‡ç­¾é¢‘æ¬¡
            tag_counts = {}
            if tag_result.data:
                for record in tag_result.data:
                    tag = record.get('tag')
                    if tag:
                        tag_counts[tag] = tag_counts.get(tag, 0) + 1
            
            # å–å‰10ä¸ªçƒ­é—¨æ ‡ç­¾
            hot_tags = [{'tag': tag, 'count': count} for tag, count in 
                       sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]]
            
            return {
                'total_news': total_result,
                'today_news': today_count,
                'week_news': week_count,
                'latest_update': latest_time,
                'hot_tags': hot_tags
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# é‡‡é›†å™¨å®ä¾‹
stock_basic_collector = StockBasicCollector()
daily_quote_collector = DailyQuoteCollector()
index_data_collector = IndexDataCollector()
trade_calendar_collector = TradeCalendarCollector()
stock_hot_rank_collector = StockHotRankCollector()
stock_hot_up_collector = StockHotUpCollector()
hsgt_fund_flow_collector = HsgtFundFlowCollector()
stock_fund_flow_rank_collector = StockFundFlowRankCollector()
stock_news_collector = StockNewsCollector() 